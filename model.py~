#-*- coding:utf-8 -*-
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf
import input_data_proto2 as input_data

#model parameters.
FLAGS = tf.app.flags.FLAGS
tf.app.flags.DEFINE_integer('batch_size', 500, 'Batch size')
tf.app.flags.DEFINE_integer('max_steps', 3000, 'Max of training step.')
tf.app.flags.DEFINE_string('train_dir', 'data', 'Directory to put the training data.')
tf.app.flags.DEFINE_string('dataset_dir', None, 'Directory of images.')

def inference(images, keep_prob):
	#重みを標準偏差0.1の正規分布で初期化
	def weight_variable(shape):
		initial = tf.truncated_normal(shape, stddev=0.1)
		return tf.Variable(initial, name='weights')

	#バイアスを0.1で初期化
	def bias_variable(shape):
		initial = tf.constant(0.1, shape=shape)
		return tf.Variable(initial, name='biases')

	#畳み込み層の作成
	def conv2d(x, W):
		return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')

	#プーリング層の作成
	def max_pool_2x2(x):
		return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')

	#畳み込み層1
	with tf.name_scope('conv1') as scope:
		W_conv1 = weight_variable([5, 5, 3, 32])
		b_conv1 = bias_variable([32])
		h_conv1 = tf.nn.relu(conv2d(images, W_conv1) + b_conv1)

	#プーリング層1
	with tf.name_scope('pool1') as scope:
		h_pool1 = max_pool_2x2(h_conv1)

	#畳み込み層2
	with tf.name_scope('conv2') as scope:
		W_conv2 = weight_variable([5, 5, 32, 64])
		b_conv2 = bias_variable([64])
		h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)

	#プーリング層2
	with tf.name_scope('pool2') as scope:
		h_pool2 = max_pool_2x2(h_conv2)

	#全結合層1
	with tf.name_scope('fc1') as scope:
		W_fc1 = weight_variable([8 * 8 * 64, 1024])
		b_fc1 = bias_variable([1024])
		h_pool2_flat = tf.reshape(h_pool2, [-1, 8 * 8 * 64])
		h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)

		#dropoutの設定
		h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

	#全結合層2
	with tf.name_scope('fc2') as scope:
		W_fc2 = weight_variable([1024, 10])
		b_fc2 = bias_variable([10])

		logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2

	return logits

def loss(logits, labels):
	cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, labels)
	loss = tf.reduce_mean(cross_entropy)
	return loss

def training(total_loss):
	train_step = tf.train.AdamOptimizer(1e-4).minimize(total_loss)
	return train_step

def evaluation(logits, labels):
	correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))
	accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
	return accuracy

def run_training():
	"""ニューラルネットワークの学習を行う
	"""
	#教師データの読み込み
	data_sets = input_data.read_data_sets(FLAGS.dataset_dir)

	with tf.Graph().as_default():
		with tf.Session() as sess:
			images_placeholder = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])
			labels_placeholder = tf.placeholder(tf.float32, shape=[None, 10])
			keep_prob = tf.placeholder(tf.float32)

			logits = inference(images_placeholder, keep_prob)
			loss_value = loss(logits, labels_placeholder)
			train_op = training(loss_value)
			accuracy = evaluation(logits, labels_placeholder)

			sess.run(tf.initialize_all_variables()) #r0.12ではtf.global_variables_initializer()を使う
			saver = tf.train.Saver()

			#学習過程を可視化するためSummaryWriterを使う
			train_writer = tf.train.SummaryWriter(FLAGS.train_dir + '/train', sess.graph) 
			test_writer = tf.train.SummaryWriter(FLAGS.train_dir + '/test') 
			loss_summary = tf.scalar_summary('loss', loss_value) 
			acc_summary = tf.scalar_summary('accuracy', accuracy) 

			#訓練
			for step in xrange(FLAGS.max_steps):
				train_images, train_labels = data_sets.train.next_batch(FLAGS.batch_size)
				feed_dict = {
					images_placeholder: train_images,
					labels_placeholder: train_labels,
					keep_prob: 0.5}
				_, cross_entropy = sess.run([train_op, loss_value], feed_dict=feed_dict)

				loss_str, acc_str = sess.run([loss_summary, acc_summary], feed_dict=feed_dict)
				train_writer.add_summary(loss_str, step)
				train_writer.add_summary(acc_str, step)

				if step%100 == 0 or (step + 1) == FLAGS.max_steps:
					#テストデータを使って評価
					feed_dict = {
						images_placeholder: data_sets.test.images,
						labels_placeholder: data_sets.test.labels,
						keep_prob: 1.0}
					test_acc, acc_str = sess.run([accuracy, acc_summary], feed_dict=feed_dict)
					test_writer.add_summary(acc_str, step)

					print('step %d, cross_entropy %g, TEST_ACCURACY %g'%(step, cross_entropy, test_acc))

					#モデルを保存
					saver.save(sess, 'model.ckpt', global_step=step)

def main(_):
	if not FLAGS.dataset_dir:
		raise ValueError('You must supply the dataset directory with --dataset_dir')
	else:
		run_training()

if __name__ == '__main__':
	tf.app.run()
